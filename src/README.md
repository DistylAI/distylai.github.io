# IFScale

A benchmark for testing language models' ability to follow instructions in text generation tasks.

## Quick Start

### Environment Setup

```bash
# 1. Create a Python 3 virtual environment (replace python3 with python3.11/3.10/etc. if you need)
python3 -m venv venv

# 2. Activate the environment
source venv/bin/activate        # macOS / Linux
# .\venv\Scripts\activate      # Windows PowerShell / CMD

# 3. Install dependencies
pip install -r requirements.txt

# 4. Set the OPENROUTER_API_KEY
export OPENROUTER_API_KEY="********"
```

### Running Experiments

#### Default Configuration (All Models)
```bash
# Run with all configured models and default parameters
python runner.py
```

#### Single Model Testing
```bash
# Test just one model
python runner.py --models "openai/gpt-4o-mini"

# Quick test with custom parameters
python runner.py --models "openai/gpt-4o-mini" --min_samples 10 --max_samples 50 --num_seeds 2
```

#### Multiple Specific Models
```bash
python runner.py --models "openai/gpt-4o-mini" "anthropic/claude-3.5-haiku"
```

#### Custom Configuration File
```bash
# Use the provided single-model example config
python runner.py --config config_single_model_example.json

# Create your own config and use it
python runner.py --config my_custom_config.json
```

#### Parameter Overrides
```bash
# Override specific parameters via CLI
python runner.py --min_samples 20 --max_samples 200 --num_seeds 3 --step_size 20

# Combine custom config with CLI overrides
python runner.py --config config_single_model_example.json --step_size 5
```

### Deactivating Environment
```bash
# When finished, leave the venv with:
deactivate
```

## Configuration System

The benchmark uses a flexible configuration system that allows easy customization without code changes.

### Configuration Files

- **`config.json`** - Main configuration with all models and default parameters
- **`config_single_model_example.json`** - Example for testing a single model quickly
- **`CONFIG_README.md`** - Detailed configuration guide

## CLI Arguments

- `--config` - Use custom configuration file
- `--models` - Override models list
- `--min_samples`, `--max_samples` - Set constraint range
- `--num_seeds` - Number of random seeds to test
- `--step_size` - Increment between constraint counts
- `--results_csv` - Custom output file path
- `--rules_csv` - Custom constraints file path

CLI arguments always take precedence over config file settings.

## Understanding Results

The benchmark tests models' ability to include specific words/constraints in coherent business reports. Results are saved as CSV files in the `outputs/results/` directory.

### Result File Structure

Each row in the results CSV contains the following columns:

- **id**: Unique identifier for each test run
- **model**: Model name (e.g., "openai/gpt-4o-mini", "anthropic/claude-3.5-haiku")
- **seed**: Random seed used for reproducibility
- **num_rules**: Number of constraint words the model needed to include
- **constraints**: Comma-separated list of required words (e.g., "voting,copayment,mechanism")
- **generated_text**: The complete business report generated by the model
- **latency_seconds**: API response time in seconds
- **coherence_score**: Quality rating from 1-10 for text coherence and readability evaluated by an LLM
- **coherence_score_reasoning**: Detailed explanation of the coherence score
- **timestamp**: When the test was executed

### Example Result Row

```csv
id,model,seed,num_rules,constraints,generated_text,latency_seconds,coherence_score,coherence_score_reasoning,timestamp
1fd9a14d-1abf-406e-9b07-26879d7f5e3c,openai/gpt-4o-mini,0,10,"voting,copayment,mechanism,subrogation,intercompany,HIPAA,streaming,whistleblower,advisory,venture","<report>### Business Report on Healthcare Innovations...[truncated]</report>",10.78,8,"Clear and well-structured overall, but the promised discussion of corporate governance voting is never developed, creating a logical gap.",2025-06-06T14:09:55.656039
```

## Analysis

The benchmark includes a comprehensive analysis module that processes experimental results to extract key performance insights and failure patterns.

### Running Analysis

After running experiments, analyze the results using:

```bash
# Analyze a specific results file and [optonally] specify custom output directory
python analysis.py --input path/to/your/results.csv --output-dir custom/analysis/path
```

The analysis script automatically processes the `generations.csv` file from your experiment runs and outputs multiple CSV files with different metrics.

### Analysis Outputs

All analysis files are saved to `outputs/analysis/` by default. The analysis generates several key metrics:

#### Core Performance Metrics

- **`accuracy_by_rules.csv`** - Mean accuracy percentage for each model at different constraint levels
- **`accuracy_std_by_rules.csv`** - Standard deviation of accuracy across test runs (measures consistency)
- **`accuracy_latency_by_rules.csv`** - Combined accuracy and response time metrics, including efficiency ratios

#### Error Analysis

- **`failure_modes_by_rules.csv`** - Breakdown of error types:
  - **omission_pct**: Percentage of errors where required words were completely missing
  - **modification_pct**: Percentage of errors where words appeared in modified form (e.g., "investment" instead of "invest")

- **`omod_ratio_by_rules.csv`** - Ratio of omission errors to modification errors (higher values indicate more complete word omissions)

#### Memory and Cognitive Effects

- **`recency_by_rules.csv`** - Recency effect analysis comparing error rates for early vs. late constraints in the list
- **`per_sample.csv`** - Detailed per-test breakdown with all computed metrics for individual samples


## Example Workflows

### Quick Model Test
```bash
# Test a single model with reduced parameters for quick feedback
python runner.py --config config_single_model_example.json
```

### Compare Two Models
```bash
# Test specific models with custom parameters
python runner.py --models "openai/gpt-4o-mini" "anthropic/claude-3.5-haiku" --max_samples 100
```

### Complete Run
```bash
# Full benchmark with all models (uses default config.json)
python runner.py
```

### Analysis results from above 
```bash
python analysis.py --input outputs/results/generations_{timestamp}.csv
```

For more detailed configuration options and advanced usage, see `CONFIG_README.md`.
